{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode Connectivity of Neural Networks (Part 1)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Intro to the Problem\n",
    "<br>\n",
    "Consider a neural network with parameters $\\theta \\in \\mathbb{R}^D$ and function $f(\\theta): \\mathbb{R}^D \\rightarrow \\mathbb{R}$ which evaluates the error/loss of the network with given set of parameters. In this view, optimization landscape is a surface defined by $(\\theta, f(\\theta))$ for all $\\theta \\in \\mathbb{R}^D$.\n",
    "\n",
    "The optimization landscape for non-convex problems, such as neural network training, typically exhibits many poor local optima and is challenging for optimization algorithms. In this project we investigate an interesting phenomenon  first described in line of works by Garipov & Draxler, called **mode connectivity**. In particular, it was shown that solutions found by SGD for large network are not isolated points in the parameter space, but rather connected via continous path on which the value of risk does not differ much from the end points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ileyeofcgjsdc52q3jildv6alkk.jpeg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "<center> Fig. 1: An example of optimization landscape of non-convex problem<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, a simple linear interpolation between the solutions is not enough and the path ought to be constructed in more intelligent fashion. In this part of the project we imprement an algorithm from **\"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs\"** by Timur Garipov et al., to find a suitable path between two solutions found by an optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training on MNIST\n",
    "\n",
    "<br>\n",
    "\n",
    "First things first, we would need to get these two solutions. In this project we consider training a simple two-layer fully connected network for the MNIST classification task, i.e., neural network should be able to assign a correct class to the input digit image.\n",
    "\n",
    "<img src=\"two-layer.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "<center> Fig. 2: Schematic illustration of two-layer neural network<center>\n",
    "<br>\n",
    "    \n",
    "To train a network and for further path construction manipulations we will rely on the Automatic Differentiation package called PyTorch, as it is relatively user-friendly and provides with all tools we would need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from copy import deepcopy\n",
    "# import \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal working examples\n",
    "\n",
    "In this section, we will try to cover the basic logics of PyTorch framework. In a nutshell, it allows to compute the derivatives for any $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ w.r.t. the input $x \\in \\mathbb{R}^d$. The input $x$ should be either `torch.Tensor` for which `requires_grad=True` or `torch.nn.Parameter`. To illustrate consider\n",
    "\n",
    "$$\n",
    "f(x) = x_1^2 + 2 x_1 x_2 + x_2^2, \\quad x \\in \\mathbb{R}^2\n",
    "$$\n",
    "\n",
    "thus the derivatives w.r.t. $x$ can be computed as follows \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1} = 2 x_1 + 2 x_2, \\quad \\frac{\\partial f}{\\partial x_2} = 2 x_2 + 2 x_1\n",
    "$$\n",
    "\n",
    "Now let us check if PyTorch would give us a proper answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 1.], requires_grad = True)  # we want derivative of f w.r.t. x in point where x = (1,1)\n",
    "f = x[0] ** 2 + 2 * x[0] * x[1] + x[1] ** 2  # define a function value at point x = (1,1)\n",
    "f.backward()  # computes a gradient w.r.t x which is stored in x.grad field\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see the output is correct since for both coordinates\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1}(1,1) = 2 x_1 + 2 x_2 = 2 \\cdot 1 + 2 \\cdot 1 = \\quad \\frac{\\partial f}{\\partial x_2}(1,1).\n",
    "$$\n",
    "\n",
    "With this in mind we can now try to implement a steapest descent to find a minimum of function $f$. Note that the global minimum is obviously allocated at point any point where $x_1 = - x_2$ since $f(x) = (x_1 + x_2)^2$. \n",
    "<br>\n",
    "\n",
    "Fortunately, many steepest descent procedures are already implemented in `torch.optim`, we will use a simple one - Gradient Descent (GD). At each step we compute the gradient at current point and then make a step in the opposite direction with some \"learning rate\" $\\alpha$, formally\n",
    "\n",
    "$$\n",
    "x^{(t + 1)} = x^{(t)} - \\alpha \\cdot \\nabla f(x^{(t)}).\n",
    "$$\n",
    "\n",
    "We can continue this procedure until convergence or just fixing a maximum amount of iterations we should perform. The example code could be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.5332e-23, 6.5332e-23], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 1.], requires_grad = True) # initialize a starting point for GD\n",
    "\n",
    "optimizer = optim.SGD([x], lr=1e-1) # define optimizer first argument is a list of variables over which\n",
    "# we want to optimize, lr = learning rate\n",
    "\n",
    "for it in range(100): # 100 is the number of GD iterations we set\n",
    "    f = x.sum().pow(2) # for each x^t we compute the corresponding value of f to get gradient after\n",
    "    # x.sum().pow(2) gives the same result as x[0] ** 2 + 2 * x[0] * x[1] + x[1] ** 2\n",
    "    f.backward() # get the gradient at current point\n",
    "    optimizer.step() # make one optimization step\n",
    "    optimizer.zero_grad() # sets the field x.grad to zero, since otherwise on next backward \n",
    "    # the gradient would be added to the previous value -> would be wrong (i.e., grads are\n",
    "    # stored always by default unless you zero them down) \n",
    "    \n",
    "print(x) # get the value of x after 100 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong? Why did we end up in a point which is approximately $(0,0)$? The answer is: GD procedure can find any stationary point, i.e., where $\\nabla f(x) = 0$. One can check that $(0,0)$ is indeed one but not a global minimum! How can we cure this? Simply by trying more initial points and picking the one which gives the best value of $f$. However since we know that in global minimum $x_1 = - x_2$ we can cheat and initialize for instance with $x=(2,-1)$ (to not get stuck at bad (0,0) point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5000, -1.5000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2., -1.], requires_grad = True) # cheating\n",
    "\n",
    "optimizer = optim.SGD([x], lr=1e-1)\n",
    "\n",
    "for it in range(100):\n",
    "    f = x.sum().pow(2)\n",
    "    f.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everying is perfect since we found a point where $x_1 = -x_2$. \n",
    "\n",
    "\n",
    "##### Neural Network Training Outline\n",
    "\n",
    "Now consider a more complicated optimization procedure â€“ neural network training. Although we will cover each step thoroughly in next section, in nutshell it can be described as follows:\n",
    "\n",
    "<br>\n",
    "Now we have a complicated function (NN) $f_{\\theta}(x)$ which depends on weights $\\theta$ and input $x$. We will focus only on classification. Consider a data $(x,y)$ ($x$ is input and $y$ is correct label for current $x$) which comes from some distribution $\\mathcal{D}$. And assume we have a measure of how good network prediction $f_{\\theta}(x)$ is compared to true $y$ - we call it $\\mathcal{L}(\\cdot, \\cdot)$. In this view, we want to find a parameters $\\theta$ which minimize the expected loss\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\mathcal{L}(f_{\\theta}(x), y),\n",
    "$$\n",
    "\n",
    "we can do so by sampling a subset of examples from $\\mathcal{D}$ of size $B$, i.e., $\\{(x_i,y_i)\\}_{i=1}^B \\sim \\mathcal{D}^B$ on each step and computing a gradient of\n",
    "\n",
    "$$\n",
    "\\frac{1}{B} \\sum_{i=1}^M \\mathcal{L}(f_{\\theta}(x_i), y_i)\n",
    "$$\n",
    "\n",
    "w.r.t. $\\theta$ and update $\\theta$ with gradient step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Dataset\n",
    "\n",
    "First we download the dataset and normalize each image for optimization stability. `train_loader` and `test_loader` are iterators for corresponding parts of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image to tensor for PyTorch, and normalize with mean and std for stability\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "# downloading training and test parts of MNIST dataset\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "\n",
    "# get train and test samplers with B=100 for train and B=1000 for test\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the sample images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAEbCAYAAADeTl6JAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbjElEQVR4nO3dd3zUVbrH8V96SEInQAglUkIvSpGyKlhARRQLIquILrZlbVdXLBc7orh2QAUVXVEUAcW2elUUGwEWpElCUUGKtGCA0FJmZv+49/W6Pr8nZDLJzGTmyef93/dwzsxxnZ08/vJwTozP53MAAAAsi63uDQAAAIQaBQ8AADCPggcAAJhHwQMAAMyj4AEAAObFl/eHZ8WO4K9w1QCfe+fGhPL1+RzVDKH8HPEZqhn4LkIwHO9zxBMeAABgHgUPAAAwj4IHAACYR8EDAADMo+ABAADmUfAAAADzKHgAAIB5FDwAAMA8Ch4AAGAeBQ8AADCPggcAAJhHwQMAAMwr9/JQoCbaMrGfyJ5ked9geue9ak1O9/nlvmabL69WY7WX1RK5yXOLK7pFAECAeMIDAADMo+ABAADmUfAAAADz6OFBjVbwcTs19mOPqQG/Tomv/D9fP+hlNfZmrwyR3/n8NJE9eZsC3gdqlpienUX++INZInd98Ua1psXD9IpZEVevrhrbMLW1yO7vngl7eqo1ay/PFtmTuzEIu4s8POEBAADmUfAAAADzKHgAAIB5FDwAAMA8mpZRo7iblL/v8XbAr/Hi/tZq7Kmcs0TOaiUPJ/ys07tqzeW1d4r8yFWNRG59J03LKN+e3nVELnU8Iqf85qebHlHNe0JzNbZ24HSR3X+hYmLjFWpN9wv7i9yCpmUAAIDoRMEDAADMo+ABAADm0cMDs0rP0Adsfdl9mmskQc15pkAewvXVyF5ywm971JrsguUixyYnizxpaVe15p5Ga0UurV+q5gDlKegme3a2lxaJ3PCVnHBuByEW30L27Jww46dq2kl04gkPAAAwj4IHAACYR8EDAADMi4genn3X9hO55Wj9e8n1e5qIXFykey8y35JjKdsPiexdlVvZLSIKHcpMVGOxrhrf3a/jOI6z6HzZb+P5ZUPA7/3TgyeKPLvBk2XMShKp+af89weOzzeghxr79rynRD7tm5tEbuusDOWWEGJb75Pn4/Q8W/4Mezzj26C8T1p/eW7Ytnvl+zZao/sLa72/LCjvHU58wwIAAPMoeAAAgHkUPAAAwDwKHgAAYF5ENC2Pv2O2yBenFuhJbSrwQgNl3FJ6RORn9w4KbGMhtGxPK5FTn6yr5sQv1Je8oeLqva4PXbtk+RUixxQcVHNKd26p8ntfc+4XIqfFJh1nJlAxv3eqpcYy4lJEzpyn/zIHotea66eIXOLzHGdm1Szq/qYc6C7je4cz1JqZhcNFjv8y8n9e8YQHAACYR8EDAADMo+ABAADmRUQPz3P3XCbyfd10HVY/zydyQccYNSex236RH+/yrshPZyxVaz4+kiby0JRDao4/R33FIi8tSlVzBiaXyAHXXtqOvF6tyV4Y8Fbghyd3Y0hed8sj8vDMsfWecM1Idtxu39lX5Npf5Ikcmt/WI1qdMU73pC04XE/ktEXykEw+Q9EjYZHuk0mIiQv6+6ws9qqxLSXpIl+Y+rvIl6bpC5MvnTVD5PMy9WXNkYYnPAAAwDwKHgAAYB4FDwAAMI+CBwAAmBcRTcup85a6sv81dSrwulOaDhR54oAs/Tpfy5vZHx/YtgKvLMUflU1gqWt2qjkNv5kvctdE183uWzgwLFrsH91PjX1/pWxSrhsrm5RzinTz4aqJ8kb1Wgej7/ZhhE5c5/YiT2r8lprzysHmInv2HwjpnhA8R4f3EfnqjLlqjvugwcocPNhl4Q0ipy/Uh6AmHZCve/dA+Sxk7Yjn/L7P9rvlDevNH11c0S2GDU94AACAeRQ8AADAPAoeAABgXkT08IRK6a7dIqfO363muH8jmjpvX5Xfd/c1usejc6L8n/qJ3+Xv57Ne/UWtKa3yThAK+Sf51Ji7Z8dtzKJr1Fj2Anp2cHw7zmrod86KwlaukaOh2QyqxN2P5TiOM/EpeXBfr8RiNcdxyj94sKxLPSd8dbHIHcevF9lzUF+Y7NZ+U7bIy87X3299ko6J/MlfHxd5cPJ4tSZrkrxg1FdU5HcvwcQTHgAAYB4FDwAAMI+CBwAAmGe6hydc4lu1EHnqPVPVHPclcHOfPVPkhjv1xYCIDMWfyz6JnA5PljFL/o67e84YkTve/rNawcWOKM/BTiV+56ya2kPkeg7fI5HIm6h/1Jbds1O+v/x6tsiFI2upOdnbZW9gZb5n3Jcsj3vtBjVn+fXPiJwRJ/fyw1j5547jOBe/K78Xfavz1JxQ4gkPAAAwj4IHAACYR8EDAADMo+ABAADm0bQcBOv/K1Pk3kkxas66YnkgWIPcIyHdEyovvnWWyA+3lZf61S/jkMEVrvOzWj0sWwU9BQVB2RvsKjqnt8jvD54i8kP5PdWaBvPXiOxVMxDN7tndS+SD18jDKD3bN4VlH1nz89XYvcP7ivxY03+HZS9VwRMeAABgHgUPAAAwj4IHAACYRw9PJRQNlb9r/+GSp10zktSav95yi8i1FnNxZKRq884OkU9M9P/fBaMWyoO5sldH/u+zEVm2ny6/jrslyl6xMVu6qjWND69XY4gO7sNoy7JGXVQcnp4dJUb3pcbHyo6xivzz/PagzE2HV2VTgeMJDwAAMI+CBwAAmEfBAwAAzKOHpxK2niPrxLQY2bMzavNZak3Kp6tFdv9mFtWjYEw/NfZgE/floPLf75gtZzpuHcf/JDIXgyJQ6V32iOzxyR6J+Pfrh3M7CKINf01RYyW+6PmW2HJRQzU2L132oZb44lxZ//M1u1/mcJ8bxRMeAABgHgUPAAAwj4IHAACYR8EDAADMo2nZj9jatdXY6FO+E/mg95jIeya1VmuSijiILhLEZzYT+ZSbl6o5abH64Mg/ysltq8ayC/j3i4qLP6GVGnuivbyk9qUDLURuMDMnpHtC6Ew45cPq3kK54ls0F7mwp/yefPHq5wN+zWVF+pLlmOLSgF8nmHjCAwAAzKPgAQAA5lHwAAAA8+jh8WPTA53V2EeN5O8zL9h0schJ/6KfI1Ll3SP7IhY09f+79UFrR4jsPmTQcThoEIHZdH0zNdbX1Tp27Q+DRG7h/BjKLaEGy32wqcjrBk8N+DXmH2ok8gt/H6HmJOdV76XZPOEBAADmUfAAAADzKHgAAIB59PC4HLiir8hrRj6n5vxcWiLyocnyDIMkZ2fwN4agWHH+066R8s/ccRzHqTtOXnFXWlAQxB2hJvK2OOZ3ztH9+hwToKoSFmWosUcz5lf5dV/b0V/k5A+rt1+nLDzhAQAA5lHwAAAA8yh4AACAeRQ8AADAvBrftOy+TPLWe+eInBSj/ye6bPVokdM/4aBBy0qa1BU5oTgzKK/r2Zsvsq+oSOSYJN1QHZfeSI2J10yvp8Y23Z4Y8N58nhiRO9xUxmGLBw8G/Lr4X8+f/IbfOZmfxIVhJwiHuBivGkuI8f/v9+Cf+5b75w8+9IoaG1Sr/Ib4st63xOc+OjXwz57v9B0Brwk3nvAAAADzKHgAAIB5FDwAAMC8GtXDExOv/3G7f7Rd5BFp+0R+s7CxWtPkXlkn6t/OwpKP580Myev2XzlK5PzddUSun16o1iztOTske/Gn04Qb1Vjr8TnVsJPodGxYH5H/lFzWoWw16uu4RnlsziVq7NKxz/hd980/pomse220El+FtxXQ67p1WXiDyO2cHwJ/4zDjCQ8AADCPggcAAJhHwQMAAMyrWb807t5eDT3ceFa5S6ZNGqHG6q2mdyFaXZB7ucgLu8yrpp04zuIT36ryaxzxFYtc4vPfUXbumqvU2IFV5Z/vk/ldaUD7grT1fNlYUdb5Xg/ldxU57f0VIleiNQMRovWcfDW27Ap5OWyfJP8XyobKsiK5lxm7ThO5YFxTtabDZnk2V+BdQOHHEx4AAGAeBQ8AADCPggcAAJhHwQMAAMwz3bQc1ylb5Oveft/vmk4z/yZy1qwlQd0TqletIZtF7jxJH6jnq8T/K2p3+F3kyhwQ2Pnbq+U+tqb6XdN63iE5sGyt3zX1nU0VGkPlxdWRh0jeOeBfftfM/uRUkVuX8pcjrPDkblRj9912jcjbhum/cLDxnOkh29MfjZspDxFs8chi14yCsOwj1HjCAwAAzKPgAQAA5lHwAAAA80z38KwfV1/kYSkH/a5pvkge5Ob4OO7LshPuCU2fxHlOz4DXnOCsCcFOUB28RUUi5x5pJvKZO3qpNe0mrRM5Gg5yQ+XVel9eIJtdRovpqaNkT2nCVbtF/rTzHLVm8I+Xiex9TV6A7YvR75O1aq/IVj97POEBAADmUfAAAADzKHgAAIB5FDwAAMA8U03Lx4b1EXnhsCddM1LCtxkANZbP1bS8wdWjnOj8qtZYbRRF5dV5y3Xw7VsyXujIn3mO4zipzi+uEXfWaspnjyc8AADAPAoeAABgHgUPAAAwz1QPz28D4kRuGe+/Z+fNQnkoU8JBefAgxw4CABD9eMIDAADMo+ABAADmUfAAAADzTPXw+PPovk5qLGdIlsi+nWvDtBsAABAuPOEBAADmUfAAAADzKHgAAIB5FDwAAMA8U03Lre/KEfncu06qwKpdodkMAACIGDzhAQAA5lHwAAAA8yh4AACAeTE+H9djAgAA23jCAwAAzKPgAQAA5lHwAAAA8yh4AACAeRQ8AADAPAoeAABgHgUPAAAwj4IHAACYR8EDAADMo+ABAADmUfAAAADzKHgAAIB5FDwAAMA8Ch4AAGAeBQ8AADCPggcAAJhHwQMAAMyj4AEAAOZR8AAAAPMoeAAAgHkUPAAAwDwKHgAAYB4FDwAAMI+CBwAAmEfBAwAAzIsv7w/Pih3hC9dGUH0+986NCeXr8zmqGUL5OeIzVDPwXYRgON7niCc8AADAPAoeAABgHgUPAAAwj4IHAACYR8EDAADMo+ABAADmUfAAAADzKHgAAIB5FDwAAMA8Ch4AAGAeBQ8AADCPggcAAJhHwQMAAMyj4AEAAOZR8AAAAPMoeAAAgHkUPAAAwDwKHgAAYB4FDwAAMI+CBwAAmEfBAwAAzIuv7g0Ek2fQSSLfOOMdkV9o1zac2xEKR/YVud6qfJE9G34K53YQAfZf2U/kpY+9IHKnaePUmpaTl4nsKy0N/sYQkPhWLdRY4zn7Rf56RSeROzwv/9xxHMezbkMwt1VpcenpamzfOfK7s/6cH9QcX1FRyPYEBANPeAAAgHkUPAAAwDwKHgAAYJ6pHp5fhySJ3CDuUDXtRNs1tFjkktGy1mxwXjh3g3CLz2ymxh6+7+Vy1+T+7Xk1ds5zp4jsKyys2sYQsPimTUR+aNF8Nad9glfk0/c1FdmzblPwN1ZJ7p6dy7/T/Tl9k98T+W9rr9cvtHJdUPcFLa5RQ5E3PN1SzRnYTn62dpxWInJN7rXiCQ8AADCPggcAAJhHwQMAAMyL2h6emIRENXb66avCv5EKqr0yWeRLx34t8lf1mqs1nv0HQronhM+eIa3U2OCUkjJm/r+Tlo9UY+mHNgZtT/AvvnmmGqs754jI3RLj1Jz2X9wgcrsxui8mUuRNzBL50rRP1ZyTnhkvcrOVi0O5JfyfPTf2F/n+W14XeWjKZ35fY3ijYSKX7vit6huLUjzhAQAA5lHwAAAA8yh4AACAeRQ8AADAvKhtWi688CQ19lzmFJE7LrhR5HbO0pDuqTxF9X0i31x/vciLanfUi2hajlqxKSkiD7n5u4BfI+nt+nrQ59NjCJmCAfpi0AVZ0/yu6zhhj8iRdMWrr193kX86b7rIp60doda0mCm/rzzB31aNF5fdRo29fPszIvdIlD+y5fGWZdv5Qm2RM65vquaU7txVgVeKfjzhAQAA5lHwAAAA8yh4AACAeVHTw+Mb0EPkaZOfVXPeOCgPd+swQR7SVp2/d+43+MdqfHeEW1F/2ZM1sfErftcc8coLZuvMXhLUPcG/+FayZ2fvBcf8run1xE1qrOm2yDiYz92v4ziOM+HNf5a75tDHuscjdd8vQdsTypZ3l+7ZK+tQy0At7Tlb5I05xWrORbNuE7n1IytF9h7z//+DaMATHgAAYB4FDwAAMI+CBwAAmEfBAwAAzIuapuWCu+UNxc3j9VFet900VOSEghUh3dPxxGfopr9XW8obiEt81JqWbb4o8GbDSzYNd43U3FuNq8u2Z9NE3tTnNTVnwp4eIme+uk7NiZSD+XYMTFVjA5LkcXVdFo8RueWUyGi4ti6uU7bIX5zxTBmzaok0eZ/8yxDL97dUK+a00bfd/1F2QqIae+nyF+T7zLxAZO/mX8t9zWjBT10AAGAeBQ8AADCPggcAAJgXkT08+67tp8bmdv2HyK8f6KbmJHxRPT07brkP6QsHS3zyt/pjtpwpsmfP3pDuCeE1tPdqv3MOeI+KXPJAE5Fj6eEJO58vRmT3/28dx3GW7ssSOe7oHjUnXGJry4shNzzSSeQF5z+l1nidBJFbjlgb/I3Br/w+DUXOik9Rc67bdqrI2/seEjk2Vfa2Oo7j9LxBHoT592vfEfny2vrzemqyzB/O3ypy7lAbF47yhAcAAJhHwQMAAMyj4AEAAOZFZA9P7PB8NdYsPknkV2afreY0d6rn/Ii4zu1FfuOM6WpOka9E5K1PyTMYUouWBn9jCJuic3uLPDXzJb9rtruOkor9emXZExFR/tVhgchjFw1Sc7YWZohc/IrugQjUrlN8auzck1eJ/EGz510zEhy3AasuE7m+s6mqW0MleOSPNMfr6H+/a6Z3FbmBkyPXHD6s1mQ8KX8OvjNMfjeNqv2R3oxPns20u0j2hvmOFek1UYgnPAAAwDwKHgAAYB4FDwAAMI+CBwAAmBcRTctx6ekiT8j+2O+a5pMi54K79ePqidwrSR9WNq1AHgiWOp8mZUt299bNof4M++hWkds5fCaqW+Mp8rLGr2YkqzmDah0T+ZWWX6k5sY48wND7lG5IDZT7NR2n7EbXP3qrsIkaa3iP/Nr3qhkIh9oX7/Q758AQ2ZTc4NXA3+e+Vh+4Rvw/5/h2ZQeRswuWBf7GEYgnPAAAwDwKHgAAYB4FDwAAMC8ienhiUuTvyYekHFBz+vz7SpGbOnkh3VMgGmX97nfOm5t7yTXOxlBtB9Ug8cSCcv88r1hf8tfhOXnApu78QrjFfykvIH72T6erOQ/3zxJ5+2DdR/PTsBdFXlYk+2+u+OyGgPfW7nV9+NvHc2eWu+bx3CFqLHP1uoDfG8FXOF8eTul01nOu6iT7+r7p3UfkvSemqTW+8+TPoy4Jsv8mr0Qegus4jtM5IVHk986ZIvKdfa/Vm1uyRo9FOJ7wAAAA8yh4AACAeRQ8AADAvIjo4fH+vl/kh/eepOb8uc1ykb/JaKPmlO7cFdR9HU98qxYif9/jbdcMXUceXdLINUIPT7Q6dl4fNba89wuukTiRNpQ0Vms8G38O5rYQAqW7dquxlHflWPa7et25N+jvMLHGCfxck9huHfSY62yeifldRG51i+6HLFUjqA5NP9gs8sa7i9WcOxrminznAtm76u8cJsdxnJE/DxX56M3pas6Fby0S+eo620T++Wb9M63NEr9vHXF4wgMAAMyj4AEAAOZR8AAAAPMoeAAAgHmR0bRcWCjyZzt0c963PWaLvPOjunrO9H5V3sv+TrIJLC1LN/31bbZFZG8Frt+LqfrdgYgQRxvFqbGEGD32R+NXXKTGTnCi7+AuVJ+t9+vPmLtp9bNHThU5bVsUdpbWEO6/ZHPdHbeqOa8+8ZTI2QmpcoJP/+xp+5k8JLDDjetF9h6WjdCO4ziPfTlM5LHD5V/CmNxLd+a/3F02Q3tXR85hwMfDEx4AAGAeBQ8AADCPggcAAJgXET08bvUfTFZjpz0wSuT3urym5ky+P6fK7728SP6e3FNGTdgr0X1AVIya49ZyylqR/Xf9IFIVDd/vd477stDmLyeEaDewKv862ZO4pu80NWdL6VGRa+3Vh9chOqTNXarGrnZuE/n3S+X3yrEDSWpNxzvkgaaew4f9vnf7u2RfzxntZM/h553nqzX33y9/NmbqNsWIwxMeAABgHgUPAAAwj4IHAACYF5E9PM6ytWqo7rkyjx54s5qzv53+fWagGr7kvw9ox7udRV5x8mt+17jPGkL0iMuWF9Uu7/1GWbNE+uSQvMQx4YsVwd4WjDty1iG/cy5ZdY3Ijb/6IVTbQTVw9/WkzfW/xlOJ93H/fDr4nvz+cuSPPMdxHGdyN9nX83zGQJHDdZl3IHjCAwAAzKPgAQAA5lHwAAAA8yh4AACAeZHZtFwBcYt0c17DReF576NbasuBk/2v8Q3oIXLM96uCth+E1u5BjUX2d1Go4zjO1K/OErmdow8VA8ozvecskXd6jqg5DZ9JCdd2UIOkT18m8snn/FnNWdpTXuh9y9+zRG5zO03LAAAAYUfBAwAAzKPgAQAA5kVtD0+1ct0VGluBupGeneh1rIH/y2FXFMlLGztO3i5yaVB3BIu2391f5AFJsk9xSZHu14njoEGEglceX9jwSf3Zy58lL67Nu0xebjts9pVqjW/FuiBsrvJ4wgMAAMyj4AEAAOZR8AAAAPPo4akMn4xex1s9+0BYND59h985Hxw8UWTP3vxQbQdGXT5qoche1xfN2OVXqTWtHHnRclzDBnJC44ZqjSdvU+U2iBor9uuVamzgP+8QOfcvsoen8BHZ4+M4jlNnhDzDLtyXavOEBwAAmEfBAwAAzKPgAQAA5lHwAAAA82hargRvcvlNyns9RWHaCUIhJilJ5Auarfa7Zl9xmsi+Ij4DCC6vR//36Z4b5WGFQ6/5VuQFv2SoNZkXBXdfqJnaztgm8qwRTUX+pus8tebs7n8ROfa7VUHfV3l4wgMAAMyj4AEAAOZR8AAAAPPo4amEN85+UeS8YtnTM+q18WpNS2dxSPeEIPLIi/Nm5P1J5Fv7b1FLFm1rK3KmU72X5MGevFNfVWPeU+XhhJ2/kT0SbR84rNZ41AgQuNJt8oLkdy48TeTRX8xRa/LvOCZy4++Cv6/y8IQHAACYR8EDAADMo+ABAADmUfAAAADzaFquhIc2ny/y4eczRW45nwblaOYrLRU56y7Z+Nnx0dFqTcyq2moMCMT//Lds+sy9Wx4amLO0g1rT4dnfRG6za4PInmOySRQIFU/eJpFH/jJYzfnwxJdFHtt3nH6hJWuCuq8/4gkPAAAwj4IHAACYR8EDAADMo4enMs6QBy6lOtuPMxEWeH7aLHLLEdW0EZiW/OEykfd+KP+8rbNErSlVI0BkOHKhT40tXdxM5IL2qWpOff0xDxqe8AAAAPMoeAAAgHkUPAAAwDx6eAAAQFB58vepsRnZrUWu7+SEazuO4/CEBwAA1AAUPAAAwDwKHgAAYB4FDwAAMI+CBwAAmEfBAwAAzKPgAQAA5lHwAAAA82J8Pn3BFwAAgCU84QEAAOZR8AAAAPMoeAAAgHkUPAAAwDwKHgAAYB4FDwAAMO8/RgnX6eYdvMkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(10,5))\n",
    "\n",
    "data = next(iter(test_loader))[0][0:8]\n",
    "for i in range(8):\n",
    "    ax[i // 4][i % 4].imshow(data[i].numpy()[0,:,:])\n",
    "    ax[i // 4][i % 4].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Network Definition\n",
    "\n",
    "For this task we will train a two-layer network with 500 hidden units. Since each MNIST image has size $28 \\times 28$ first linear layer has shape $(28 \\cdot 28) \\times 500$. As the number of classes for digits is equal to $10$ final layer has shape $500 \\times 10$. As an activation function we will use Rectified Linear Unit (`nn.ReLU`). **The network outputs the vector of size 10 which corresponds to probability of each class**.\n",
    "\n",
    "\n",
    "**Task1:** Implement a forward pass for this network which consists of\n",
    "\n",
    "$$\n",
    "\\textrm{input} \\rightarrow \\textrm{Linear layer 1} \\rightarrow \\textrm{ReLU} \\rightarrow \\textrm{Linear layer 2}\n",
    "$$\n",
    "\n",
    "in corresponding function. Each layer or activation output on input $x$ can be obtained calling\n",
    "\n",
    "$$\n",
    "\\textrm{layer/act}(x)\n",
    "$$\n",
    "\n",
    "The forward pass for the model on input is called in the same fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All network parameters can be accessed via `.parameters()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Training Epoch\n",
    "\n",
    "For classification task we will use $\\mathcal{L}$ which is cross-entropy loss. It can be accessed by calling `F.cross_entropy(output, target)`, where `output` is the result of forward pass through the model and `target` are true labels.\n",
    "\n",
    "**Task1:**\n",
    "In cell bellow implement one step of optimization proceedure:\n",
    "\n",
    "*1)* Obtain output for current subset of dataset, i.e., `data`\n",
    "\n",
    "*2)* Compute loss objective on current `output` and `target`\n",
    "\n",
    "*3)* Compute the gradient update and make a step\n",
    "\n",
    "**Task2:** In addition, it is helpful to see that the loss is decreasing during iterations. Thus, implement progress tracker which will print the loss value periodically (say each 60 steps <=> `batch_idx % 60 == 0` ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): # sample subset of dataset - (x,y)\n",
    "        # input\n",
    "        data = data.view(-1,28 * 28) # reshape 2d image to vector of size 28x28\n",
    "        \n",
    "        # => Implement a computation of gradient for current batch of data\n",
    "\n",
    "        #forward propagation\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         output = model.forward(data)\n",
    "        #lost function\n",
    "        L = F.cross_entropy(output, target)\n",
    "        #back propagation \n",
    "        L.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        # => Implement progress tracker\n",
    "        running_loss += L.item()\n",
    "        if batch_idx % 60 == 59:\n",
    "            print(\"Loss at step {} is {}\".format(batch_idx+1, running_loss/60))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Test epoch\n",
    "\n",
    "As model is trained on the training part of the dataset a good performance measurement is to track loss and accuracy\n",
    "\n",
    "$$\n",
    "\\textrm{accuracy(output, target)} =  \\mathbb{I}\\{\\arg\\max_{i}\\textrm{output}_i = target\\},\n",
    "$$\n",
    "\n",
    "i.e., if max probability is assigned to a correct class, which model achieves on a test part of the dataset which it did not see.\n",
    "\n",
    "\n",
    "**Task 1:** Compute the loss objective and accuracy averaged over the whole test dataset. Print it and return average test loss value as function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, print_par = False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad(): # to not store the gradients (i.e., ease memory requirement)\n",
    "        for data, target in test_loader:\n",
    "            data = data.view(-1,28 * 28)\n",
    "    # ===> Implement progress tracker\n",
    "#             x = torch.tensor(data, requires_grad = True)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            accuracy += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy /= len(test_loader.dataset)\n",
    "    if print_par == True:\n",
    "        print('Average loss on the test data: ', test_loss)\n",
    "        print('Average accuracy on the test data: {:.2f}%'.format(100* accuracy))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5:  Train two networks for connectivity experiments\n",
    "\n",
    "Now we have both training and test functions in our hands. Train function implements one pass over the whole dataset.\n",
    "\n",
    "**Task 1:** Train first network to test accuracy around **97-98%**.\n",
    "\n",
    "*1)* Define optimizer over network parameters with some learning rate.\n",
    "\n",
    "*2) (optional)* For better results use `StepLR` (step learning rate) to decay learning rate after each epoch\n",
    "\n",
    "*3)* After each training call in loop call test function to see resulting model performance\n",
    "\n",
    "*4)* Number of epochs (i.e., train calls) is up to you, but around 15 should be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.7573317711551985\n",
      "Loss at step 120 is 0.35663134654363\n",
      "Loss at step 180 is 0.2981538546582063\n",
      "Loss at step 240 is 0.2769935503602028\n",
      "Loss at step 300 is 0.236839152003328\n",
      "Loss at step 360 is 0.2175705809146166\n",
      "Loss at step 420 is 0.1995322725425164\n",
      "Loss at step 480 is 0.17811228185892106\n",
      "Loss at step 540 is 0.17451139905800422\n",
      "Loss at step 600 is 0.16325414975484212\n",
      "Epoch  2 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.14401906511435905\n",
      "Loss at step 120 is 0.13668231113503376\n",
      "Loss at step 180 is 0.1390680868178606\n",
      "Loss at step 240 is 0.13733451291918755\n",
      "Loss at step 300 is 0.12299841543038686\n",
      "Loss at step 360 is 0.12386636137962341\n",
      "Loss at step 420 is 0.12009356037403146\n",
      "Loss at step 480 is 0.10784564279019833\n",
      "Loss at step 540 is 0.11562192977095644\n",
      "Loss at step 600 is 0.11460419694582621\n",
      "Epoch  3 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.08645307465145985\n",
      "Loss at step 120 is 0.08961861729621887\n",
      "Loss at step 180 is 0.0884548170802494\n",
      "Loss at step 240 is 0.0873181341526409\n",
      "Loss at step 300 is 0.09906469595928986\n",
      "Loss at step 360 is 0.08378113806247711\n",
      "Loss at step 420 is 0.08328074871872862\n",
      "Loss at step 480 is 0.08231916377941767\n",
      "Loss at step 540 is 0.07859918453420202\n",
      "Loss at step 600 is 0.08709133708228668\n",
      "Epoch  4 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.06351912350704272\n",
      "Loss at step 120 is 0.06966877191637953\n",
      "Loss at step 180 is 0.065467812679708\n",
      "Loss at step 240 is 0.0595928568392992\n",
      "Loss at step 300 is 0.06329760841714839\n",
      "Loss at step 360 is 0.06660471462334196\n",
      "Loss at step 420 is 0.06749407801156243\n",
      "Loss at step 480 is 0.0693971062079072\n",
      "Loss at step 540 is 0.06650439615671834\n",
      "Loss at step 600 is 0.06346960285057625\n",
      "Epoch  5 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.0478680898124973\n",
      "Loss at step 120 is 0.0516019859816879\n",
      "Loss at step 180 is 0.05595626261395713\n",
      "Loss at step 240 is 0.05298143364489079\n",
      "Loss at step 300 is 0.0487876847696801\n",
      "Loss at step 360 is 0.05028220564126969\n",
      "Loss at step 420 is 0.04928795006126165\n",
      "Loss at step 480 is 0.050569475069642066\n",
      "Loss at step 540 is 0.05254084633973737\n",
      "Loss at step 600 is 0.04884139923378825\n",
      "Epoch  6 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.03152787759900093\n",
      "Loss at step 120 is 0.04552934692862133\n",
      "Loss at step 180 is 0.04073725991571943\n",
      "Loss at step 240 is 0.041330236840682724\n",
      "Loss at step 300 is 0.038458272768184545\n",
      "Loss at step 360 is 0.0366331049396346\n",
      "Loss at step 420 is 0.0440134313578407\n",
      "Loss at step 480 is 0.040190956881269814\n",
      "Loss at step 540 is 0.04266338039500018\n",
      "Loss at step 600 is 0.045673396849694355\n",
      "Epoch  7 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.035779473410608865\n",
      "Loss at step 120 is 0.03532342598773539\n",
      "Loss at step 180 is 0.032760503919174275\n",
      "Loss at step 240 is 0.02992461233710249\n",
      "Loss at step 300 is 0.03279400315756599\n",
      "Loss at step 360 is 0.03229439821249495\n",
      "Loss at step 420 is 0.02883092740861078\n",
      "Loss at step 480 is 0.03990406511972348\n",
      "Loss at step 540 is 0.034269979557332894\n",
      "Loss at step 600 is 0.03366693204734474\n",
      "Epoch  8 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.0251914320125555\n",
      "Loss at step 120 is 0.025046460120938717\n",
      "Loss at step 180 is 0.02536463289676855\n",
      "Loss at step 240 is 0.02841650101666649\n",
      "Loss at step 300 is 0.025347947189584375\n",
      "Loss at step 360 is 0.0307523344643414\n",
      "Loss at step 420 is 0.028033611302574476\n",
      "Loss at step 480 is 0.026741493896891674\n",
      "Loss at step 540 is 0.029594815491388243\n",
      "Loss at step 600 is 0.02816917543920378\n",
      "Epoch  9 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.018991382652893664\n",
      "Loss at step 120 is 0.022128003587325414\n",
      "Loss at step 180 is 0.023275738954544066\n",
      "Loss at step 240 is 0.022655393447106084\n",
      "Loss at step 300 is 0.02031943140706668\n",
      "Loss at step 360 is 0.022992710497540734\n",
      "Loss at step 420 is 0.02275361733045429\n",
      "Loss at step 480 is 0.021067531313747168\n",
      "Loss at step 540 is 0.02694037021137774\n",
      "Loss at step 600 is 0.024199675217581294\n",
      "Epoch  10 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.018782501225359738\n",
      "Loss at step 120 is 0.016301378809536495\n",
      "Loss at step 180 is 0.018578110348122814\n",
      "Loss at step 240 is 0.015575257048476487\n",
      "Loss at step 300 is 0.015875140988888838\n",
      "Loss at step 360 is 0.01938408794812858\n",
      "Loss at step 420 is 0.02131318262933443\n",
      "Loss at step 480 is 0.020818503713235258\n",
      "Loss at step 540 is 0.020764939727572103\n",
      "Loss at step 600 is 0.019496450115305684\n",
      "Epoch  11 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.013252398457067709\n",
      "Loss at step 120 is 0.014413605739052097\n",
      "Loss at step 180 is 0.015096345532219857\n",
      "Loss at step 240 is 0.016717157404248912\n",
      "Loss at step 300 is 0.01782627486390993\n",
      "Loss at step 360 is 0.013518184233301629\n",
      "Loss at step 420 is 0.017111693656382463\n",
      "Loss at step 480 is 0.014775647028970221\n",
      "Loss at step 540 is 0.015008534215545903\n",
      "Loss at step 600 is 0.017830568187249204\n",
      "Epoch  12 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.012788280553650111\n",
      "Loss at step 120 is 0.014268984513667723\n",
      "Loss at step 180 is 0.010092300114532311\n",
      "Loss at step 240 is 0.0123756522545591\n",
      "Loss at step 300 is 0.011192699254024774\n",
      "Loss at step 360 is 0.01404401072456191\n",
      "Loss at step 420 is 0.012021649464926062\n",
      "Loss at step 480 is 0.01534270791647335\n",
      "Loss at step 540 is 0.014395824198921521\n",
      "Loss at step 600 is 0.013023266568779945\n",
      "Epoch  13 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.010224583651870489\n",
      "Loss at step 120 is 0.009554775365783523\n",
      "Loss at step 180 is 0.010255963072025528\n",
      "Loss at step 240 is 0.010531435590625431\n",
      "Loss at step 300 is 0.010000243198980267\n",
      "Loss at step 360 is 0.01210488670427973\n",
      "Loss at step 420 is 0.012413970058939109\n",
      "Loss at step 480 is 0.011827316282627483\n",
      "Loss at step 540 is 0.011341369101622452\n",
      "Loss at step 600 is 0.011816251437024524\n",
      "Epoch  14 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.008377572257692615\n",
      "Loss at step 120 is 0.00830285432554471\n",
      "Loss at step 180 is 0.007957400164256494\n",
      "Loss at step 240 is 0.008629160417088617\n",
      "Loss at step 300 is 0.008397409975683938\n",
      "Loss at step 360 is 0.009975797543302179\n",
      "Loss at step 420 is 0.010580163123086095\n",
      "Loss at step 480 is 0.009681556416520227\n",
      "Loss at step 540 is 0.008990898414049297\n",
      "Loss at step 600 is 0.011419650064393256\n",
      "Epoch  15 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.007968421940070887\n",
      "Loss at step 120 is 0.007310084596974775\n",
      "Loss at step 180 is 0.008966114344851424\n",
      "Loss at step 240 is 0.007431105516540508\n",
      "Loss at step 300 is 0.007854372561754037\n",
      "Loss at step 360 is 0.007001225949109843\n",
      "Loss at step 420 is 0.009506658899287384\n",
      "Loss at step 480 is 0.009185991424601525\n",
      "Loss at step 540 is 0.008304601570125669\n",
      "Loss at step 600 is 0.00808431402547285\n"
     ]
    }
   ],
   "source": [
    "model = FCN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1) \n",
    "# ===> Implement training\n",
    "n_epoch = 15\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "# ===> Train + test step with optional StepLR call\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    print('Epoch ', epoch, '. Learning rate ', scheduler.get_last_lr())\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    av_test_loss = test(model, test_loader, print_par = False)\n",
    "    scheduler.step()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save this model for later connectivity experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Train the second model for path construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.7087538247307141\n",
      "Loss at step 120 is 0.36207369367281594\n",
      "Loss at step 180 is 0.29521420150995253\n",
      "Loss at step 240 is 0.2692855400343736\n",
      "Loss at step 300 is 0.24753297629455726\n",
      "Loss at step 360 is 0.21809135004878044\n",
      "Loss at step 420 is 0.210921265433232\n",
      "Loss at step 480 is 0.20057628552118936\n",
      "Loss at step 540 is 0.1840683575719595\n",
      "Loss at step 600 is 0.16390062825133403\n",
      "Average loss on the test data:  0.00015469815768301488\n",
      "Average accuracy on the test data: 95.52%\n",
      "Epoch  2 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.14505403159807126\n",
      "Loss at step 120 is 0.13648819290101527\n",
      "Loss at step 180 is 0.13850642281274\n",
      "Loss at step 240 is 0.131428709688286\n",
      "Loss at step 300 is 0.1369518888493379\n",
      "Loss at step 360 is 0.12489037066698075\n",
      "Loss at step 420 is 0.11800014947851499\n",
      "Loss at step 480 is 0.11606702785938978\n",
      "Loss at step 540 is 0.10987854776903987\n",
      "Loss at step 600 is 0.10437291450798511\n",
      "Average loss on the test data:  0.00010389374084770679\n",
      "Average accuracy on the test data: 96.96%\n",
      "Epoch  3 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.09909402647366126\n",
      "Loss at step 120 is 0.09928926303982735\n",
      "Loss at step 180 is 0.08280675895512105\n",
      "Loss at step 240 is 0.09386422305057446\n",
      "Loss at step 300 is 0.08911127367367347\n",
      "Loss at step 360 is 0.07811971080179016\n",
      "Loss at step 420 is 0.08286114316433668\n",
      "Loss at step 480 is 0.0818801086395979\n",
      "Loss at step 540 is 0.08408029197404782\n",
      "Loss at step 600 is 0.07106234260524312\n",
      "Average loss on the test data:  8.290317971259355e-05\n",
      "Average accuracy on the test data: 97.52%\n",
      "Epoch  4 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.06664837024485072\n",
      "Loss at step 120 is 0.05990723731617133\n",
      "Loss at step 180 is 0.07092744279652834\n",
      "Loss at step 240 is 0.06452810519064466\n",
      "Loss at step 300 is 0.06533371458450953\n",
      "Loss at step 360 is 0.06260363338515162\n",
      "Loss at step 420 is 0.07469077991942565\n",
      "Loss at step 480 is 0.06197158150995771\n",
      "Loss at step 540 is 0.059781914545844\n",
      "Loss at step 600 is 0.06611167440811792\n",
      "Average loss on the test data:  7.389386482536793e-05\n",
      "Average accuracy on the test data: 97.78%\n",
      "Epoch  5 . Learning rate  [0.1]\n",
      "Loss at step 60 is 0.05198679892346263\n",
      "Loss at step 120 is 0.04799971460985641\n",
      "Loss at step 180 is 0.05487212655134499\n",
      "Loss at step 240 is 0.043319015422215064\n",
      "Loss at step 300 is 0.05615875832736492\n",
      "Loss at step 360 is 0.05673082723903159\n",
      "Loss at step 420 is 0.04836985409880678\n",
      "Loss at step 480 is 0.05121615114621818\n",
      "Loss at step 540 is 0.05071497614650677\n",
      "Loss at step 600 is 0.05012690584796171\n",
      "Average loss on the test data:  6.955966167151928e-05\n",
      "Average accuracy on the test data: 97.89%\n",
      "Epoch  6 . Learning rate  [0.08000000000000002]\n",
      "Loss at step 60 is 0.03696608869358897\n",
      "Loss at step 120 is 0.03081877822987735\n",
      "Loss at step 180 is 0.04297153010653953\n",
      "Loss at step 240 is 0.03644796133351823\n",
      "Loss at step 300 is 0.04056221073648582\n",
      "Loss at step 360 is 0.04548561186529696\n",
      "Loss at step 420 is 0.0371413041682293\n",
      "Loss at step 480 is 0.04336404347947488\n",
      "Loss at step 540 is 0.04475644343377402\n",
      "Loss at step 600 is 0.04052590769715607\n",
      "Average loss on the test data:  6.375846974551677e-05\n",
      "Average accuracy on the test data: 97.94%\n",
      "Epoch  7 . Learning rate  [0.08000000000000002]\n",
      "Loss at step 60 is 0.03115378920920193\n",
      "Loss at step 120 is 0.028339202088924747\n",
      "Loss at step 180 is 0.036983859810667735\n",
      "Loss at step 240 is 0.03533125232594709\n",
      "Loss at step 300 is 0.0348516987481465\n",
      "Loss at step 360 is 0.03526270045743634\n",
      "Loss at step 420 is 0.03458908606941501\n",
      "Loss at step 480 is 0.03062169187081357\n",
      "Loss at step 540 is 0.037478419059577085\n",
      "Loss at step 600 is 0.02984004852672418\n",
      "Average loss on the test data:  6.382485367357731e-05\n",
      "Average accuracy on the test data: 98.07%\n",
      "Epoch  8 . Learning rate  [0.08000000000000002]\n",
      "Loss at step 60 is 0.02915842067450285\n",
      "Loss at step 120 is 0.028859997432058058\n",
      "Loss at step 180 is 0.028785613598302007\n",
      "Loss at step 240 is 0.030251841999900837\n",
      "Loss at step 300 is 0.027197387263489265\n",
      "Loss at step 360 is 0.027555541911472876\n",
      "Loss at step 420 is 0.026965626305900513\n",
      "Loss at step 480 is 0.029073427198454738\n",
      "Loss at step 540 is 0.028207329233797887\n",
      "Loss at step 600 is 0.026780755437600116\n",
      "Average loss on the test data:  5.99360104650259e-05\n",
      "Average accuracy on the test data: 98.07%\n",
      "Epoch  9 . Learning rate  [0.08000000000000002]\n",
      "Loss at step 60 is 0.019980634787740807\n",
      "Loss at step 120 is 0.026794287438193957\n",
      "Loss at step 180 is 0.022700800816528498\n",
      "Loss at step 240 is 0.02242206052566568\n",
      "Loss at step 300 is 0.025923262373544274\n",
      "Loss at step 360 is 0.02497265060277035\n",
      "Loss at step 420 is 0.026343304951054355\n",
      "Loss at step 480 is 0.025584757421165706\n",
      "Loss at step 540 is 0.026694418381278714\n",
      "Loss at step 600 is 0.021623173418144386\n",
      "Average loss on the test data:  5.8407986350357535e-05\n",
      "Average accuracy on the test data: 98.11%\n",
      "Epoch  10 . Learning rate  [0.08000000000000002]\n",
      "Loss at step 60 is 0.015630600275471807\n",
      "Loss at step 120 is 0.020932889675411084\n",
      "Loss at step 180 is 0.019598558467502396\n",
      "Loss at step 240 is 0.021682787934939066\n",
      "Loss at step 300 is 0.01934274413312475\n",
      "Loss at step 360 is 0.02159733509955307\n",
      "Loss at step 420 is 0.023922688296685615\n",
      "Loss at step 480 is 0.01979171809895585\n",
      "Loss at step 540 is 0.022627029404975473\n",
      "Loss at step 600 is 0.02222964952234179\n",
      "Average loss on the test data:  5.928667113184929e-05\n",
      "Average accuracy on the test data: 98.14%\n",
      "Epoch  11 . Learning rate  [0.06400000000000002]\n",
      "Loss at step 60 is 0.018515569119093318\n",
      "Loss at step 120 is 0.017990500231583915\n",
      "Loss at step 180 is 0.017655364315335948\n",
      "Loss at step 240 is 0.01613052178096647\n",
      "Loss at step 300 is 0.01625321297130237\n",
      "Loss at step 360 is 0.0161973348314253\n",
      "Loss at step 420 is 0.014868206961546093\n",
      "Loss at step 480 is 0.018535390162530044\n",
      "Loss at step 540 is 0.01754833918530494\n",
      "Loss at step 600 is 0.017997792122575143\n",
      "Average loss on the test data:  5.671210046857595e-05\n",
      "Average accuracy on the test data: 98.20%\n",
      "Epoch  12 . Learning rate  [0.06400000000000002]\n",
      "Loss at step 60 is 0.014160051456807802\n",
      "Loss at step 120 is 0.013889066549018026\n",
      "Loss at step 180 is 0.01354104537749663\n",
      "Loss at step 240 is 0.013908706558868288\n",
      "Loss at step 300 is 0.016919561894610524\n",
      "Loss at step 360 is 0.018979878160947313\n",
      "Loss at step 420 is 0.015447234633999567\n",
      "Loss at step 480 is 0.013172633938180904\n",
      "Loss at step 540 is 0.016960482369177042\n",
      "Loss at step 600 is 0.0148496319539845\n",
      "Average loss on the test data:  5.679306508973241e-05\n",
      "Average accuracy on the test data: 98.33%\n",
      "Epoch  13 . Learning rate  [0.06400000000000002]\n",
      "Loss at step 60 is 0.013578837372673054\n",
      "Loss at step 120 is 0.013104328326880932\n",
      "Loss at step 180 is 0.015514073714924356\n",
      "Loss at step 240 is 0.010203885232719282\n",
      "Loss at step 300 is 0.012049496538626652\n",
      "Loss at step 360 is 0.015788123263822246\n",
      "Loss at step 420 is 0.014587681325307737\n",
      "Loss at step 480 is 0.013946071775474895\n",
      "Loss at step 540 is 0.012445673843224843\n",
      "Loss at step 600 is 0.016746778467980523\n",
      "Average loss on the test data:  5.530273327603936e-05\n",
      "Average accuracy on the test data: 98.22%\n",
      "Epoch  14 . Learning rate  [0.06400000000000002]\n",
      "Loss at step 60 is 0.012141447747126222\n",
      "Loss at step 120 is 0.010915687528904528\n",
      "Loss at step 180 is 0.012167339539155364\n",
      "Loss at step 240 is 0.010175543476361782\n",
      "Loss at step 300 is 0.012581318787609538\n",
      "Loss at step 360 is 0.012575289614809056\n",
      "Loss at step 420 is 0.013871829659910873\n",
      "Loss at step 480 is 0.014817600794291745\n",
      "Loss at step 540 is 0.010584078973624855\n",
      "Loss at step 600 is 0.012805844323399167\n",
      "Average loss on the test data:  5.646657571196556e-05\n",
      "Average accuracy on the test data: 98.15%\n",
      "Epoch  15 . Learning rate  [0.06400000000000002]\n",
      "Loss at step 60 is 0.011791931232437491\n",
      "Loss at step 120 is 0.011483100359328091\n",
      "Loss at step 180 is 0.009501785839286942\n",
      "Loss at step 240 is 0.011452965366576489\n",
      "Loss at step 300 is 0.010165113105904311\n",
      "Loss at step 360 is 0.012359233483827363\n",
      "Loss at step 420 is 0.01051095996517688\n",
      "Loss at step 480 is 0.011354847900414218\n",
      "Loss at step 540 is 0.010960781380223732\n",
      "Loss at step 600 is 0.01142133284593001\n",
      "Average loss on the test data:  5.596435582265258e-05\n",
      "Average accuracy on the test data: 98.27%\n"
     ]
    }
   ],
   "source": [
    "model = FCN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1) \n",
    "# ===> Implement training\n",
    "n_epoch = 15\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "\n",
    "# ===> Train + test step with optional StepLR call\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    print('Epoch ', epoch, '. Learning rate ', scheduler.get_last_lr())\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    av_test_loss = test(model, test_loader, print_par = True)\n",
    "    scheduler.step()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving resulting model for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path construction\n",
    "\n",
    "In this section we will constuct a simple two-segment linear path of low loss between two solutions. The path between network parameters $\\theta_1$ and $\\theta_2$ has the following parameteric form\n",
    "\n",
    "$$\n",
    "\\pi(t) = \n",
    "\\begin{cases} 2((0.5-t)\\theta_1 + t\\theta), \\quad t \\in [0,0.5] \\\\\n",
    "2(t\\theta_2 + (0.5-t)\\theta), \\quad t \\in [0.5,1] \n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "We aim to find $\\theta$ such that the value of risk for any $t \\in [0,1]$ is small, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\mathcal{L}(f_{\\pi(t)}(x), y) \\leq \\max[ \\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\mathcal{L}(f_{\\theta_1}(x), y), \\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\mathcal{L}(f_{\\theta_2}(x), y)] + \\varepsilon,\n",
    "$$\n",
    "\n",
    "does not exceed the value of both end network plus some small value. How we can achieve that? The work around is quite simple:\n",
    "\n",
    "*1)* for each part of training dataset, i.e., `(data, target)` pair from one iteration over `train_loader` sample $t \\sim \\textrm{Uniform[0,1]}$\n",
    "\n",
    "*2)* Compute the loss on this pair for network with parameters $\\pi(t)$\n",
    "\n",
    "*3)* Compute derivatives w.r.t. $\\theta$ to minimize the loss and make a step of optimizer\n",
    "\n",
    "*4)* Repeat above proceedure for several epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Technical note:** Due to restrictions of PyTorch we will use `CurveFCN` class below to insert the parameter values from the path in order to allow backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight, bias):\n",
    "        super(Linear, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "class CurveFCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight_1, bias_1, weight_2, bias_2):\n",
    "        super(CurveFCN, self).__init__()\n",
    "        self.fc1 = Linear(weight_1, bias_1)\n",
    "        self.fc2 = Linear(weight_2, bias_2)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.act(self.fc1(x))\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Given $\\theta_1 = \\textrm{start}$, $\\theta_2 = \\textrm{end}$, $\\theta = \\textrm{middle}$ and time step \n",
    "$t$ implement a function below which returns a new point which corresponds to $\\pi(t)$. \n",
    "\n",
    "\n",
    "**Note:** Strictly speaking each $\\pi(t)$ consists of 4 parameters `(weight_1, bias_1)` for weights and bias term of first linear layer and `(weight_2, bias_2)` for parameters of second. In this function we assume that we get a consistent triplet, e.g., weights of first layer. In next task we will call this function on all triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain_element(t, start, middle, end):\n",
    "    if 0 <= t <= 0.5:\n",
    "        pi = 2*((0.5-t)*start + t*middle)\n",
    "    elif 0.5 < t <= 1:\n",
    "        pi = 2*(t*end + (0.5-t)*middle)\n",
    "    else: print('error in pi')\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we emplemented a function which outputs a network with parameters sampled from path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(t, model_1, model_mid, model_2):\n",
    "\n",
    "    params_start = list(model_1.parameters())\n",
    "    params_end = list(model_2.parameters())\n",
    "    params_middle = model_mid\n",
    "    \n",
    "    params = []\n",
    "    for idx in range(len(params_start)):\n",
    "        start, middle, end = params_start[idx], params_middle[idx], params_end[idx]\n",
    "        new_param = get_chain_element(t, start, middle, end)\n",
    "        params.append(new_param)\n",
    "    new_model = CurveFCN(*params)\n",
    "        \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Implement a training algorithm for $\\theta$. \n",
    "\n",
    "For a certain amount of epochs go through the dataset and on each subset of training dataset sample a timestemp $t$. \n",
    "\n",
    "After use `get_net(t, model_1, model_mid, model_2)` to obtain a network model with parameters $\\pi(t)$. For which compute a loss on current subset and gradient for optimization step.\n",
    "\n",
    "**Note:** you are free to tune optimizer parameters and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 500])\n"
     ]
    }
   ],
   "source": [
    "print(list(model_1.parameters())[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on the test data:  7.706583198159933e-05\n",
      "Average accuracy on the test data: 97.59%\n",
      "Average loss on the test data:  0.00012271766448393464\n",
      "Average accuracy on the test data: 98.19%\n"
     ]
    }
   ],
   "source": [
    "model_mid = list(FCN().parameters())\n",
    "\n",
    "optimizer = optim.Adam(model_mid, lr=0.0001, weight_decay=1e-7)\n",
    "\n",
    "for epoch in range(1, 2 + 1): #54\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        t = torch.rand(1)[0] # sample timestemp\n",
    "        # input\n",
    "        data = data.view(-1,28 * 28) # reshape 2d image to vector of size 28x28\n",
    "\n",
    "        # get model and make an optimization step\n",
    "        new_model = get_net(t, model_1, model_mid, model_2)\n",
    "        output = new_model(data)\n",
    "        #loss function\n",
    "        L = F.cross_entropy(output, target)\n",
    "        #back propagation \n",
    "        L.backward()\n",
    "        optimizer.step() # make one optimization step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    test_loss = test(new_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Implement a function which computes test loss for 50 timesteps in $[0,1]$ interval and returns resulting array of losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(model_1, model_mid, model_2):\n",
    "    timesteps = np.linspace(0, 1, 50)\n",
    "    \n",
    "    path = np.empty(len(timesteps))\n",
    "    # iterate over test data, output loss and test\n",
    "    for i,t in enumerate(timesteps):\n",
    "        print(i, t)\n",
    "        new_model = get_net(t, model_1, model_mid, model_2)\n",
    "        path[i] = test(new_model, test_loader)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "Average loss on the test data:  5.8176336577162144e-05\n",
      "Average accuracy on the test data: 98.21%\n",
      "1 0.02040816326530612\n",
      "Average loss on the test data:  5.802972042001784e-05\n",
      "Average accuracy on the test data: 98.21%\n",
      "2 0.04081632653061224\n",
      "Average loss on the test data:  5.8335139509290455e-05\n",
      "Average accuracy on the test data: 98.19%\n",
      "3 0.061224489795918366\n",
      "Average loss on the test data:  5.9150540363043544e-05\n",
      "Average accuracy on the test data: 98.14%\n",
      "4 0.08163265306122448\n",
      "Average loss on the test data:  6.051256321370602e-05\n",
      "Average accuracy on the test data: 98.09%\n",
      "5 0.1020408163265306\n",
      "Average loss on the test data:  6.253802143037319e-05\n",
      "Average accuracy on the test data: 98.03%\n",
      "6 0.12244897959183673\n",
      "Average loss on the test data:  6.535607734695077e-05\n",
      "Average accuracy on the test data: 97.97%\n",
      "7 0.14285714285714285\n",
      "Average loss on the test data:  6.910741766914725e-05\n",
      "Average accuracy on the test data: 97.87%\n",
      "8 0.16326530612244897\n",
      "Average loss on the test data:  7.39230290055275e-05\n",
      "Average accuracy on the test data: 97.80%\n",
      "9 0.18367346938775508\n",
      "Average loss on the test data:  7.992603238672018e-05\n",
      "Average accuracy on the test data: 97.63%\n",
      "10 0.2040816326530612\n",
      "Average loss on the test data:  8.725406378507614e-05\n",
      "Average accuracy on the test data: 97.34%\n",
      "11 0.22448979591836732\n",
      "Average loss on the test data:  9.601951986551285e-05\n",
      "Average accuracy on the test data: 97.19%\n",
      "12 0.24489795918367346\n",
      "Average loss on the test data:  0.00010633747056126595\n",
      "Average accuracy on the test data: 96.99%\n",
      "13 0.26530612244897955\n",
      "Average loss on the test data:  0.0001183618526905775\n",
      "Average accuracy on the test data: 96.70%\n",
      "14 0.2857142857142857\n",
      "Average loss on the test data:  0.00013213769197463989\n",
      "Average accuracy on the test data: 96.38%\n",
      "15 0.3061224489795918\n",
      "Average loss on the test data:  0.00014767409786581992\n",
      "Average accuracy on the test data: 96.02%\n",
      "16 0.32653061224489793\n",
      "Average loss on the test data:  0.00016486890241503715\n",
      "Average accuracy on the test data: 95.59%\n",
      "17 0.3469387755102041\n",
      "Average loss on the test data:  0.00018352464362978934\n",
      "Average accuracy on the test data: 95.07%\n",
      "18 0.36734693877551017\n",
      "Average loss on the test data:  0.00020349983870983124\n",
      "Average accuracy on the test data: 94.55%\n",
      "19 0.3877551020408163\n",
      "Average loss on the test data:  0.00022456685528159142\n",
      "Average accuracy on the test data: 94.02%\n",
      "20 0.4081632653061224\n",
      "Average loss on the test data:  0.00024647183790802953\n",
      "Average accuracy on the test data: 93.43%\n",
      "21 0.42857142857142855\n",
      "Average loss on the test data:  0.00026901410594582555\n",
      "Average accuracy on the test data: 92.94%\n",
      "22 0.44897959183673464\n",
      "Average loss on the test data:  0.00029220332205295564\n",
      "Average accuracy on the test data: 92.28%\n",
      "23 0.4693877551020408\n",
      "Average loss on the test data:  0.00031625218391418455\n",
      "Average accuracy on the test data: 91.65%\n",
      "24 0.4897959183673469\n",
      "Average loss on the test data:  0.0003415918216109276\n",
      "Average accuracy on the test data: 91.07%\n",
      "25 0.5102040816326531\n",
      "Average loss on the test data:  5.705710491165519e-05\n",
      "Average accuracy on the test data: 98.29%\n",
      "26 0.5306122448979591\n",
      "Average loss on the test data:  5.836699325591326e-05\n",
      "Average accuracy on the test data: 98.31%\n",
      "27 0.5510204081632653\n",
      "Average loss on the test data:  6.011074613779783e-05\n",
      "Average accuracy on the test data: 98.31%\n",
      "28 0.5714285714285714\n",
      "Average loss on the test data:  6.224347054958344e-05\n",
      "Average accuracy on the test data: 98.33%\n",
      "29 0.5918367346938775\n",
      "Average loss on the test data:  6.47115744650364e-05\n",
      "Average accuracy on the test data: 98.30%\n",
      "30 0.6122448979591836\n",
      "Average loss on the test data:  6.749870097264647e-05\n",
      "Average accuracy on the test data: 98.28%\n",
      "31 0.6326530612244897\n",
      "Average loss on the test data:  7.055226787924767e-05\n",
      "Average accuracy on the test data: 98.28%\n",
      "32 0.6530612244897959\n",
      "Average loss on the test data:  7.382686361670494e-05\n",
      "Average accuracy on the test data: 98.27%\n",
      "33 0.673469387755102\n",
      "Average loss on the test data:  7.735870452597738e-05\n",
      "Average accuracy on the test data: 98.27%\n",
      "34 0.6938775510204082\n",
      "Average loss on the test data:  8.111511301249266e-05\n",
      "Average accuracy on the test data: 98.28%\n",
      "35 0.7142857142857142\n",
      "Average loss on the test data:  8.509423583745957e-05\n",
      "Average accuracy on the test data: 98.24%\n",
      "36 0.7346938775510203\n",
      "Average loss on the test data:  8.92815925180912e-05\n",
      "Average accuracy on the test data: 98.25%\n",
      "37 0.7551020408163265\n",
      "Average loss on the test data:  9.367173481732606e-05\n",
      "Average accuracy on the test data: 98.20%\n",
      "38 0.7755102040816326\n",
      "Average loss on the test data:  9.824112858623267e-05\n",
      "Average accuracy on the test data: 98.19%\n",
      "39 0.7959183673469387\n",
      "Average loss on the test data:  0.00010302951894700527\n",
      "Average accuracy on the test data: 98.20%\n",
      "40 0.8163265306122448\n",
      "Average loss on the test data:  0.00010802953252568841\n",
      "Average accuracy on the test data: 98.18%\n",
      "41 0.836734693877551\n",
      "Average loss on the test data:  0.00011322618070989848\n",
      "Average accuracy on the test data: 98.17%\n",
      "42 0.8571428571428571\n",
      "Average loss on the test data:  0.00011861601267009974\n",
      "Average accuracy on the test data: 98.20%\n",
      "43 0.8775510204081632\n",
      "Average loss on the test data:  0.00012419941741973161\n",
      "Average accuracy on the test data: 98.20%\n",
      "44 0.8979591836734693\n",
      "Average loss on the test data:  0.0001299760663881898\n",
      "Average accuracy on the test data: 98.21%\n",
      "45 0.9183673469387754\n",
      "Average loss on the test data:  0.00013594135669991374\n",
      "Average accuracy on the test data: 98.17%\n",
      "46 0.9387755102040816\n",
      "Average loss on the test data:  0.0001420929567888379\n",
      "Average accuracy on the test data: 98.17%\n",
      "47 0.9591836734693877\n",
      "Average loss on the test data:  0.00014842754397541286\n",
      "Average accuracy on the test data: 98.16%\n",
      "48 0.9795918367346939\n",
      "Average loss on the test data:  0.00015495452042669057\n",
      "Average accuracy on the test data: 98.16%\n",
      "49 1.0\n",
      "Average loss on the test data:  0.00016167444400489331\n",
      "Average accuracy on the test data: 98.14%\n"
     ]
    }
   ],
   "source": [
    "res = get_path(model_1, model_mid, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.81763366e-05, 5.80297204e-05, 5.83351395e-05, 5.91505404e-05,\n",
       "       6.05125632e-05, 6.25380214e-05, 6.53560773e-05, 6.91074177e-05,\n",
       "       7.39230290e-05, 7.99260324e-05, 8.72540638e-05, 9.60195199e-05,\n",
       "       1.06337471e-04, 1.18361853e-04, 1.32137692e-04, 1.47674098e-04,\n",
       "       1.64868902e-04, 1.83524644e-04, 2.03499839e-04, 2.24566855e-04,\n",
       "       2.46471838e-04, 2.69014106e-04, 2.92203322e-04, 3.16252184e-04,\n",
       "       3.41591822e-04, 5.70571049e-05, 5.83669933e-05, 6.01107461e-05,\n",
       "       6.22434705e-05, 6.47115745e-05, 6.74987010e-05, 7.05522679e-05,\n",
       "       7.38268636e-05, 7.73587045e-05, 8.11151130e-05, 8.50942358e-05,\n",
       "       8.92815925e-05, 9.36717348e-05, 9.82411286e-05, 1.03029519e-04,\n",
       "       1.08029533e-04, 1.13226181e-04, 1.18616013e-04, 1.24199417e-04,\n",
       "       1.29976066e-04, 1.35941357e-04, 1.42092957e-04, 1.48427544e-04,\n",
       "       1.54954520e-04, 1.61674444e-04])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** To compare our learn path we choose a simple linear interpolation between two points, namely\n",
    "\n",
    "$$\n",
    "\\pi_{\\textrm{linear}}(t) = (1-t)\\theta_1 + t \\theta_2,\\quad t \\in [0,1]\n",
    "$$\n",
    "\n",
    "Implement it in function `get_linear_element` in similar spirit to `get_chain_element`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_element(t, start, end):\n",
    "    return (1-t)*start + t*end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_net(t, model_1, model_2):\n",
    "\n",
    "    params_start = list(model_1.parameters())\n",
    "    params_end = list(model_2.parameters())\n",
    "    \n",
    "    params = []\n",
    "    for idx in range(len(params_start)):\n",
    "        start, end = params_start[idx], params_end[idx]\n",
    "        new_param = get_linear_element(t, start, end)\n",
    "        params.append(new_param)\n",
    "    new_model = CurveFCN(*params)\n",
    "        \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** Similar to **task 3** implement a function which returns a loss values for 50 timesteps which partition $[0,1]$ interval on equal pieces but for **linear interpolation** (call `get_linear_net`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_linear(model_1, model_mid, model_2):  \n",
    "    # => implement\n",
    "    timesteps = np.linspace(0, 1, 50)\n",
    "    \n",
    "    path = np.empty(len(timesteps))\n",
    "    # iterate over test data, output loss and test\n",
    "    for i,t in enumerate(timesteps):\n",
    "        new_model = get_linear_net(t, model_1, model_2)\n",
    "        path[i] = test(new_model, test_loader)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on the test data:  5.8176336577162144e-05\n",
      "Average accuracy on the test data: 98.21%\n",
      "Average loss on the test data:  5.773191428743303e-05\n",
      "Average accuracy on the test data: 98.21%\n",
      "Average loss on the test data:  5.75730268843472e-05\n",
      "Average accuracy on the test data: 98.20%\n",
      "Average loss on the test data:  5.775975687429309e-05\n",
      "Average accuracy on the test data: 98.10%\n",
      "Average loss on the test data:  5.834875134751201e-05\n",
      "Average accuracy on the test data: 98.04%\n",
      "Average loss on the test data:  5.937627535313368e-05\n",
      "Average accuracy on the test data: 97.99%\n",
      "Average loss on the test data:  6.0904159024357794e-05\n",
      "Average accuracy on the test data: 97.95%\n",
      "Average loss on the test data:  6.297813272103667e-05\n",
      "Average accuracy on the test data: 97.90%\n",
      "Average loss on the test data:  6.566813811659812e-05\n",
      "Average accuracy on the test data: 97.88%\n",
      "Average loss on the test data:  6.907070316374301e-05\n",
      "Average accuracy on the test data: 97.84%\n",
      "Average loss on the test data:  7.329278159886598e-05\n",
      "Average accuracy on the test data: 97.79%\n",
      "Average loss on the test data:  7.845211811363697e-05\n",
      "Average accuracy on the test data: 97.69%\n",
      "Average loss on the test data:  8.466371763497591e-05\n",
      "Average accuracy on the test data: 97.59%\n",
      "Average loss on the test data:  9.19818252325058e-05\n",
      "Average accuracy on the test data: 97.48%\n",
      "Average loss on the test data:  0.00010048619024455547\n",
      "Average accuracy on the test data: 97.32%\n",
      "Average loss on the test data:  0.00011014912985265255\n",
      "Average accuracy on the test data: 97.18%\n",
      "Average loss on the test data:  0.00012080198302865029\n",
      "Average accuracy on the test data: 97.02%\n",
      "Average loss on the test data:  0.00013219261392951012\n",
      "Average accuracy on the test data: 96.88%\n",
      "Average loss on the test data:  0.00014393682032823562\n",
      "Average accuracy on the test data: 96.75%\n",
      "Average loss on the test data:  0.0001555831976234913\n",
      "Average accuracy on the test data: 96.52%\n",
      "Average loss on the test data:  0.0001666217967867851\n",
      "Average accuracy on the test data: 96.29%\n",
      "Average loss on the test data:  0.00017637340426445008\n",
      "Average accuracy on the test data: 96.14%\n",
      "Average loss on the test data:  0.00018424628898501395\n",
      "Average accuracy on the test data: 95.98%\n",
      "Average loss on the test data:  0.00018971574306488037\n",
      "Average accuracy on the test data: 95.91%\n",
      "Average loss on the test data:  0.0001924318864941597\n",
      "Average accuracy on the test data: 95.91%\n",
      "Average loss on the test data:  0.00019222490713000299\n",
      "Average accuracy on the test data: 95.92%\n",
      "Average loss on the test data:  0.0001890918493270874\n",
      "Average accuracy on the test data: 95.98%\n",
      "Average loss on the test data:  0.00018324858471751212\n",
      "Average accuracy on the test data: 96.05%\n",
      "Average loss on the test data:  0.00017509363740682602\n",
      "Average accuracy on the test data: 96.26%\n",
      "Average loss on the test data:  0.00016513977348804474\n",
      "Average accuracy on the test data: 96.39%\n",
      "Average loss on the test data:  0.00015401561409235002\n",
      "Average accuracy on the test data: 96.51%\n",
      "Average loss on the test data:  0.00014227543398737908\n",
      "Average accuracy on the test data: 96.68%\n",
      "Average loss on the test data:  0.0001304803229868412\n",
      "Average accuracy on the test data: 96.84%\n",
      "Average loss on the test data:  0.00011913031749427319\n",
      "Average accuracy on the test data: 96.98%\n",
      "Average loss on the test data:  0.00010851544626057148\n",
      "Average accuracy on the test data: 97.17%\n",
      "Average loss on the test data:  9.88715272396803e-05\n",
      "Average accuracy on the test data: 97.41%\n",
      "Average loss on the test data:  9.034550301730633e-05\n",
      "Average accuracy on the test data: 97.53%\n",
      "Average loss on the test data:  8.297148123383523e-05\n",
      "Average accuracy on the test data: 97.66%\n",
      "Average loss on the test data:  7.67132854089141e-05\n",
      "Average accuracy on the test data: 97.81%\n",
      "Average loss on the test data:  7.150983698666095e-05\n",
      "Average accuracy on the test data: 97.87%\n",
      "Average loss on the test data:  6.728881504386664e-05\n",
      "Average accuracy on the test data: 97.92%\n",
      "Average loss on the test data:  6.38772837817669e-05\n",
      "Average accuracy on the test data: 97.99%\n",
      "Average loss on the test data:  6.117951590567828e-05\n",
      "Average accuracy on the test data: 98.06%\n",
      "Average loss on the test data:  5.909228892996907e-05\n",
      "Average accuracy on the test data: 98.12%\n",
      "Average loss on the test data:  5.7550850417464974e-05\n",
      "Average accuracy on the test data: 98.20%\n",
      "Average loss on the test data:  5.65217487514019e-05\n",
      "Average accuracy on the test data: 98.25%\n",
      "Average loss on the test data:  5.5949596874415875e-05\n",
      "Average accuracy on the test data: 98.28%\n",
      "Average loss on the test data:  5.580261200666428e-05\n",
      "Average accuracy on the test data: 98.31%\n",
      "Average loss on the test data:  5.6022274401038885e-05\n",
      "Average accuracy on the test data: 98.28%\n",
      "Average loss on the test data:  5.658488692715764e-05\n",
      "Average accuracy on the test data: 98.27%\n"
     ]
    }
   ],
   "source": [
    "res_lin = get_path_linear(model_1, model_mid, model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** Plot loss on linear path and two-segment one on a single plot.\n",
    "\n",
    "**Hint:** if everything is correct the loss on two-segment one should be significantly smaller than of linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1307aafd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "t = np.linspace(0, 1, 50)\n",
    "plt.plot(t, res, label = 'path')\n",
    "plt.plot(t, res_lin, label = 'linear interpolation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# => plot loss on paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus part) \n",
    "\n",
    "Try to get similar results for the path defined by the **Bezier curve**. It has the following parametric form\n",
    "\n",
    "$$\n",
    "\\pi(t) = (1-t^2)\\theta_1 + t^2 \\theta_2 + 2t(1-t) \\theta, \\quad t \\in [0,1],\n",
    "$$\n",
    "\n",
    "$\\theta$ is parameter which needs to be trained to get a path of low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => implementation follows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
